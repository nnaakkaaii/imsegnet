{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nakai-yu/dev/imsegnet/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchmetrics\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
    "from albumentations.pytorch import ToTensorV2 as ToTensor\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, utils\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "TRAIN_PATH = '../data/stage1_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像データ拡張の関数\n",
    "def get_train_transform():\n",
    "   return A.Compose(\n",
    "       [\n",
    "        # リサイズ(こちらはすでに適用済みなのでなくても良いです)\n",
    "        A.Resize(256, 256),\n",
    "        # 正規化(こちらの細かい値はalbumentations.augmentations.transforms.Normalizeのデフォルトの値を適用)\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        # 水平フリップ（pはフリップする確率）\n",
    "        A.HorizontalFlip(p=0.25),\n",
    "        # 垂直フリップ\n",
    "        A.VerticalFlip(p=0.25),\n",
    "        ToTensor()\n",
    "        ])\n",
    "\n",
    "# Datasetクラスの定義\n",
    "class LoadDataSet(Dataset):\n",
    "    WIDTH = 256\n",
    "    HEIGHT = 256\n",
    "\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.folders = os.listdir(path)\n",
    "        self.transforms = get_train_transform()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_folder = os.path.join(self.path, self.folders[idx], 'images/')\n",
    "        mask_folder = os.path.join(self.path, self.folders[idx], 'masks/')\n",
    "        image_path = os.path.join(image_folder, os.listdir(image_folder)[0])\n",
    "\n",
    "        # 画像データの取得\n",
    "        img = io.imread(image_path)[:, :, :3].astype('float32')\n",
    "        img = transform.resize(img, (self.WIDTH, self.HEIGHT))\n",
    "\n",
    "        mask = self.get_mask(mask_folder, self.WIDTH, self.HEIGHT).astype('float32')\n",
    "\n",
    "        augmented = self.transforms(image=img, mask=mask)\n",
    "\n",
    "        img = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "\n",
    "        mask = mask.permute(2, 0, 1)\n",
    "        point = self.get_point()\n",
    "\n",
    "        label = mask[:, point[0], point[1]]\n",
    "        point = torch.tensor([point], dtype=torch.int64)\n",
    "\n",
    "        return img, point, label\n",
    "\n",
    "    def get_mask(self, mask_folder, IMG_HEIGHT, IMG_WIDTH):\n",
    "        mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool_)\n",
    "        for mask_ in os.listdir(mask_folder):\n",
    "                mask_ = io.imread(os.path.join(mask_folder, mask_))\n",
    "                mask_ = transform.resize(mask_, (IMG_HEIGHT, IMG_WIDTH))\n",
    "                mask_ = np.expand_dims(mask_, axis=-1)\n",
    "                mask = np.maximum(mask, mask_)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def get_point(self):\n",
    "        x = random.randint(0, self.WIDTH - 1)\n",
    "        y = random.randint(0, self.HEIGHT - 1)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LoadDataSet(TRAIN_PATH, transform=get_train_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "image, point, label = train_dataset.__getitem__(0)\n",
    "print(image.shape)\n",
    "print(point.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image(img):\n",
    "    img = np.array(np.transpose(img, (1, 2, 0)))\n",
    "    # 下は画像拡張での正規化を元に戻しています\n",
    "    mean = np.array((0.485, 0.456, 0.406))\n",
    "    std = np.array((0.229, 0.224, 0.225))\n",
    "    img = std * img + mean\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data: 502\n",
      "Length of validation data: 168\n"
     ]
    }
   ],
   "source": [
    "split_ratio = 0.25\n",
    "train_size=int(np.round(train_dataset.__len__() * (1 - split_ratio), 0))\n",
    "valid_size=int(np.round(train_dataset.__len__() * split_ratio, 0))\n",
    "train_data, valid_data = random_split(train_dataset, [train_size, valid_size])\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valid_data, batch_size=10)\n",
    "\n",
    "print(\"Length of train data: {}\".format(len(train_data)))\n",
    "print(\"Length of validation data: {}\".format(len(valid_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super().__init__()\n",
    "        # 資料中の『FCN』に当たる部分\n",
    "        self.conv1 = conv_bn_relu(input_channels,64)\n",
    "        self.conv2 = conv_bn_relu(64, 128)\n",
    "        self.conv3 = conv_bn_relu(128, 256)\n",
    "        self.conv4 = conv_bn_relu(256, 512)\n",
    "        self.conv5 = conv_bn_relu(512, 1024)\n",
    "        self.down_pooling = nn.MaxPool2d(2)\n",
    "\n",
    "        # 資料中の『Up Sampling』に当たる部分\n",
    "        self.up_pool6 = up_pooling(1024, 512)\n",
    "        self.conv6 = conv_bn_relu(1024, 512)\n",
    "        self.up_pool7 = up_pooling(512, 256)\n",
    "        self.conv7 = conv_bn_relu(512, 256)\n",
    "        self.up_pool8 = up_pooling(256, 128)\n",
    "        self.conv8 = conv_bn_relu(256, 128)\n",
    "        self.up_pool9 = up_pooling(128, 64)\n",
    "        self.conv9 = conv_bn_relu(128, 64)\n",
    "        self.conv10 = nn.Conv2d(64, output_channels, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, points):\n",
    "        # 正規化\n",
    "        x = x/255.\n",
    "\n",
    "        # 資料中の『FCN』に当たる部分\n",
    "        x1 = self.conv1(x)\n",
    "        p1 = self.down_pooling(x1)\n",
    "        x2 = self.conv2(p1)\n",
    "        p2 = self.down_pooling(x2)\n",
    "        x3 = self.conv3(p2)\n",
    "        p3 = self.down_pooling(x3)\n",
    "        x4 = self.conv4(p3)\n",
    "        p4 = self.down_pooling(x4)\n",
    "        x5 = self.conv5(p4)\n",
    "\n",
    "        # 資料中の『Up Sampling』に当たる部分, torch.catによりSkip Connectionをしている\n",
    "        p6 = self.up_pool6(x5)\n",
    "        x6 = torch.cat([p6, x4], dim=1)\n",
    "        x6 = self.conv6(x6)\n",
    "\n",
    "        p7 = self.up_pool7(x6)\n",
    "        x7 = torch.cat([p7, x3], dim=1)\n",
    "        x7 = self.conv7(x7)\n",
    "\n",
    "        p8 = self.up_pool8(x7)\n",
    "        x8 = torch.cat([p8, x2], dim=1)\n",
    "        x8 = self.conv8(x8)\n",
    "\n",
    "        p9 = self.up_pool9(x8)\n",
    "        x9 = torch.cat([p9, x1], dim=1)\n",
    "        x9 = self.conv9(x9)\n",
    "\n",
    "        output = self.conv10(x9)\n",
    "\n",
    "        b, c, h, w = output.shape\n",
    "        index = (points[:, :, 0] + w * points[:, :, 1]).unsqueeze(2)\n",
    "        pred = output.reshape(b, c, h * w).gather(2, index).squeeze(2)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "def down_pooling():\n",
    "    return nn.MaxPool2d(2)\n",
    "\n",
    "def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n",
    "    return nn.Sequential(\n",
    "        # 転置畳み込み\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1]) torch.Size([10, 1])\n",
      "tensor([[  3.1918],\n",
      "        [ -0.2844],\n",
      "        [-10.2568],\n",
      "        [  2.4563],\n",
      "        [  0.2921],\n",
      "        [ -1.7849],\n",
      "        [  1.6839],\n",
      "        [ -4.5875],\n",
      "        [  0.6325],\n",
      "        [  2.1613]], grad_fn=<SqueezeBackward1>) tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [1.0000],\n",
      "        [0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def test_unet():\n",
    "    model = UNet(3, 1)\n",
    "    x, p, t = next(iter(train_loader))\n",
    "    y = model(x, p)\n",
    "    print(y.shape, t.shape)\n",
    "    print(y, t)\n",
    "    return\n",
    "\n",
    "test_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "description:   0%|          | 0/51 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Encountered different devices in metric calculation (see stacktrace for details).This could be due to the metric class not being on the same device as input.Instead of `metric=Accuracy(...)` try to do `metric=Accuracy(...).to(device)` where device corresponds to the device of the input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/imsegnet/.venv/lib/python3.9/site-packages/torchmetrics/metric.py:391\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/dev/imsegnet/.venv/lib/python3.9/site-packages/torchmetrics/classification/accuracy.py:253\u001b[0m, in \u001b[0;36mAccuracy.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduce \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmdmc_reduce \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamplewise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtp \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tp\n\u001b[1;32m    254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m fp\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mセル10 を /home/nakai-yu/dev/imsegnet/notebook/nuclei_pixel.ipynb\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubuntu/home/nakai-yu/dev/imsegnet/notebook/nuclei_pixel.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m losses_value \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubuntu/home/nakai-yu/dev/imsegnet/notebook/nuclei_pixel.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# 精度評価\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bubuntu/home/nakai-yu/dev/imsegnet/notebook/nuclei_pixel.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m score \u001b[39m=\u001b[39m accuracy_metric(y_train, t_train\u001b[39m.\u001b[39;49mlong())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubuntu/home/nakai-yu/dev/imsegnet/notebook/nuclei_pixel.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bubuntu/home/nakai-yu/dev/imsegnet/notebook/nuclei_pixel.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/dev/imsegnet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/imsegnet/.venv/lib/python3.9/site-packages/torchmetrics/metric.py:245\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_full_state_update(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    244\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_reduce_state_update(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    247\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/dev/imsegnet/.venv/lib/python3.9/site-packages/torchmetrics/metric.py:309\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    310\u001b[0m batch_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute()\n\u001b[1;32m    312\u001b[0m \u001b[39m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/imsegnet/.venv/lib/python3.9/site-packages/torchmetrics/metric.py:394\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    393\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected all tensors to be on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n\u001b[0;32m--> 394\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mEncountered different devices in metric calculation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m (see stacktrace for details).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThis could be due to the metric class not being on the same device as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInstead of `metric=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m(...)` try to do\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `metric=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m(...).to(device)` where\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m device corresponds to the device of the input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    402\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    404\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_on_cpu:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Encountered different devices in metric calculation (see stacktrace for details).This could be due to the metric class not being on the same device as input.Instead of `metric=Accuracy(...)` try to do `metric=Accuracy(...).to(device)` where device corresponds to the device of the input."
     ]
    }
   ],
   "source": [
    "# <---------------各インスタンス作成---------------------->\n",
    "model = UNet(3,1).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "accuracy_metric = torchmetrics.Accuracy(threshold=0.5).to(device='cuda:0')\n",
    "num_epochs=20\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "checkpoint_path = 'model/chkpoint_'\n",
    "best_model_path = 'model/bestmodel.pt'\n",
    "\n",
    "total_train_loss = []\n",
    "total_train_score = []\n",
    "total_valid_loss = []\n",
    "total_valid_score = []\n",
    "\n",
    "losses_value = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # <---------------トレーニング---------------------->\n",
    "    train_loss = []\n",
    "    train_score = []\n",
    "    valid_loss = []\n",
    "    valid_score = []\n",
    "    pbar = tqdm(train_loader, desc = 'description')\n",
    "    for x_train, p_train, t_train in pbar:\n",
    "        x_train = torch.autograd.Variable(x_train).cuda()\n",
    "        p_train = torch.autograd.Variable(p_train).cuda()\n",
    "        t_train = torch.autograd.Variable(t_train).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        y_train = model(x_train, p_train)\n",
    "        # 損失計算\n",
    "        loss = criterion(y_train, t_train)\n",
    "        losses_value = loss.item()\n",
    "        # 精度評価\n",
    "        score = accuracy_metric(y_train, t_train.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(losses_value)\n",
    "        train_score.append(score.item())\n",
    "        pbar.set_description(f\"Epoch: {epoch+1}, loss: {losses_value}, IoU: {score}\")\n",
    "\n",
    "    # <---------------評価---------------------->\n",
    "    with torch.no_grad():\n",
    "        for x_val, p_val, t_val in val_loader:\n",
    "            x_val = torch.autograd.Variable(x_val).cuda()\n",
    "            p_val = torch.autograd.Variable(p_val).cuda()\n",
    "            t_val = torch.autograd.Variable(t_val).cuda()\n",
    "            y_val = model(x_val, p_val)\n",
    "            # 損失計算\n",
    "            loss = criterion(y_val, t_val)\n",
    "            losses_value = loss.item()\n",
    "            # 精度評価\n",
    "            score = accuracy_metric(y_val, t_val.long())\n",
    "            valid_loss.append(losses_value)\n",
    "            valid_score.append(score.item())\n",
    "\n",
    "    total_train_loss.append(np.mean(train_loss))\n",
    "    total_train_score.append(np.mean(train_score))\n",
    "    total_valid_loss.append(np.mean(valid_loss))\n",
    "    total_valid_score.append(np.mean(valid_score))\n",
    "    print(f\"Train Loss: {total_train_loss[-1]}, Train IOU: {total_train_score[-1]}\")\n",
    "    print(f\"Valid Loss: {total_valid_loss[-1]}, Valid IOU: {total_valid_score[-1]}\")\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'valid_loss_min': total_valid_loss[-1],\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(1)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.set_style(style=\"darkgrid\")\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(x=range(1,num_epochs+1), y=total_train_loss, label=\"Train Loss\")\n",
    "sns.lineplot(x=range(1,num_epochs+1), y=total_valid_loss, label=\"Valid Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"DiceLoss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(x=range(1,num_epochs+1), y=total_train_score, label=\"Train Score\")\n",
    "sns.lineplot(x=range(1,num_epochs+1), y=total_valid_score, label=\"Valid Score\")\n",
    "plt.title(\"Score (IoU)\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predict(model, n_images):\n",
    "    figure, ax = plt.subplots(nrows=n_images, ncols=3, figsize=(15, 18))\n",
    "    with torch.no_grad():\n",
    "        for data,mask in val_loader:\n",
    "            data = torch.autograd.Variable(data, volatile=True).cuda()\n",
    "            mask = torch.autograd.Variable(mask, volatile=True).cuda()\n",
    "            o = model(data)\n",
    "            break\n",
    "    for img_no in range(0, n_images):\n",
    "        tm=o[img_no][0].data.cpu().numpy()\n",
    "        img = data[img_no].data.cpu()\n",
    "        msk = mask[img_no].data.cpu()\n",
    "        img = format_image(img)\n",
    "        msk = format_mask(msk)\n",
    "        ax[img_no, 0].imshow(img)\n",
    "        ax[img_no, 1].imshow(msk, interpolation=\"nearest\", cmap=\"gray\")\n",
    "        ax[img_no, 2].imshow(tm, interpolation=\"nearest\", cmap=\"gray\")\n",
    "        ax[img_no, 0].set_title(\"Input Image\")\n",
    "        ax[img_no, 1].set_title(\"Label Mask\")\n",
    "        ax[img_no, 2].set_title(\"Predicted Mask\")\n",
    "        ax[img_no, 0].set_axis_off()\n",
    "        ax[img_no, 1].set_axis_off()\n",
    "        ax[img_no, 2].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predict(model, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ac290080bb7a1b4650e412c63df3cbe039f928155369ccd9fe5b103f3311272"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
